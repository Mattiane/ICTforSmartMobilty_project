{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo as pm #import MongoClient only\n",
    "import ast\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import array\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import pandas as pd\n",
    "client = pm.MongoClient('bigdatadb.polito.it',\n",
    "ssl=True,\n",
    "authSource = 'carsharing',\n",
    "username = 'ictts',\n",
    "password ='Ict4SM22!',\n",
    "tlsAllowInvalidCertificates=True)\n",
    "db = client['carsharing'] #Choose the DB to use\n",
    "Bookings_col = db['ictts_PermanentBookings'] # Collection for Car2go to use\n",
    "Enjoy_Bookings_col = db['ictts_enjoy_PermanentBookings'] # Collection for Car2go to use\n",
    "\n",
    "with open('data/IMQ/Zone/TorinoZonesArray.geojson') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "zones = [ast.literal_eval(line)[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bookings_col.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: checking if data is already filtered\n",
    "booking_duration = []\n",
    "for doc in Bookings_col.find({\"city\": \"Torino\"}):\n",
    "    booking_duration.append(doc[\"final_time\"]- doc[\"init_time\"])\n",
    "    \n",
    "booking_durations_np = np.array(booking_duration)/60\n",
    "sorted_durations = np.sort(booking_durations_np)\n",
    "\n",
    "cumulative_probabilities = np.arange(1, len(sorted_durations) + 1) / len(sorted_durations)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sorted_durations, cumulative_probabilities, label=\"Torino\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Booking Duration (minutes)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.title(\"Empirical CDF of Booking Durations\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: OD matrix of all bookings\n",
    "od_matrix = np.zeros((len(zones), len(zones)), dtype=int)\n",
    "for i, origin in enumerate(zones):\n",
    "    result = []  \n",
    "    for j, dest in enumerate(zones):\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$match\": { \n",
    "                    \"init_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": origin \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": dest  \n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$count\": \"tot\"  \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        aggregation_result = list(Bookings_col.aggregate(pipeline))\n",
    "        if aggregation_result:\n",
    "            od_matrix[i, j] = aggregation_result[0][\"tot\"]\n",
    "        else:\n",
    "            od_matrix[i, j] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#od_amtrix enjoy\n",
    "od_matrix_e = np.zeros((len(zones), len(zones)), dtype=int)\n",
    "for i, origin in enumerate(zones):\n",
    "    result = []  \n",
    "    for j, dest in enumerate(zones):\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$match\": { \n",
    "                    \"init_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": origin \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": dest  \n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$count\": \"tot\"  \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        aggregation_result = list(Enjoy_Bookings_col.aggregate(pipeline))\n",
    "        if aggregation_result:\n",
    "            od_matrix_e[i, j] = aggregation_result[0][\"tot\"]\n",
    "        else:\n",
    "            od_matrix_e[i, j] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = np.sum(od_matrix_e)\n",
    "od_matrix_norm_e = od_matrix_e / tot_sum\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(od_matrix_norm_e, cmap=\"YlGnBu\", cbar=True, xticklabels=range(1, len(zones)+1), yticklabels=range(1, len(zones)+1))\n",
    "\n",
    "plt.title(\"OD Matrix Non Filtered Heat Map Enjoy\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: Heat map plot\n",
    "tot_sum = np.sum(od_matrix)\n",
    "od_matrix_norm = od_matrix / tot_sum\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(od_matrix_norm, cmap=\"YlGnBu\", cbar=True, xticklabels=range(1, len(zones)+1), yticklabels=range(1, len(zones)+1))\n",
    "\n",
    "plt.title(\"OD Matrix Non Filtered Heat Map\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1a: filtering data by weekdays\n",
    "od_matrix_weekdays = np.zeros((len(zones), len(zones)), dtype=int)\n",
    "for i, origin in enumerate(zones):\n",
    "    result = []  \n",
    "    for j, dest in enumerate(zones):\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    #\"hour\": { \"$hour\": \"$init_date\" },\n",
    "                    \"day\": { \"$dayOfWeek\": \"$init_date\" },\n",
    "                    \"init_loc\": 1,\n",
    "                    \"final_loc\": 1\n",
    "                } \n",
    "            },\n",
    "            {\n",
    "                \"$match\": { \n",
    "                    \"day\": { \n",
    "                        \"$gt\": 1, \n",
    "                        \"$lt\":7 },\n",
    "                    \"init_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": origin \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": dest  \n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$count\": \"tot\"  \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        aggregation_result = list(Bookings_col.aggregate(pipeline))\n",
    "        if aggregation_result:\n",
    "            od_matrix_weekdays[i, j] = aggregation_result[0][\"tot\"]\n",
    "        else:\n",
    "            od_matrix_weekdays[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = np.sum(od_matrix_weekdays)\n",
    "od_matrix_weekdays_norm = od_matrix_weekdays / tot_sum\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(od_matrix_weekdays_norm, cmap=\"YlGnBu\", cbar=True, xticklabels=range(1, len(zones)+1), yticklabels=range(1, len(zones)+1))\n",
    "\n",
    "plt.title(\"OD Matrix Weekdays Heat Map\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1a: filtering data by weekends\n",
    "od_matrix_weekends = np.zeros((len(zones), len(zones)), dtype=int)\n",
    "for i, origin in enumerate(zones):\n",
    "    result = []  \n",
    "    for j, dest in enumerate(zones):\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    #\"hour\": { \"$hour\": \"$init_date\" },\n",
    "                    \"day\": { \"$dayOfWeek\": \"$init_date\" },\n",
    "                    \"init_loc\": 1,\n",
    "                    \"final_loc\": 1\n",
    "                } \n",
    "            },\n",
    "            {\n",
    "                \"$match\": { \n",
    "                    \"day\": { \n",
    "                        \"$in\": [1, 7]},\n",
    "                    \"init_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": origin \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": dest  \n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$count\": \"tot\"  \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        aggregation_result = list(Bookings_col.aggregate(pipeline))\n",
    "        if aggregation_result:\n",
    "            od_matrix_weekends[i, j] = aggregation_result[0][\"tot\"]\n",
    "        else:\n",
    "            od_matrix_weekends[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = np.sum(od_matrix_weekends)\n",
    "od_matrix_weekends_norm = od_matrix_weekends / tot_sum\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(od_matrix_weekends_norm, cmap=\"YlGnBu\", cbar=True, xticklabels=range(1, len(zones)+1), yticklabels=range(1, len(zones)+1))\n",
    "\n",
    "plt.title(\"OD Matrix Weekends Heat Map\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": None,\n",
    "            \"minDate\": { \"$min\": \"$init_date\" }, \n",
    "            \"maxDate\": { \"$max\": \"$init_date\" }   \n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "result = list(Enjoy_Bookings_col.aggregate(pipeline))\n",
    "\n",
    "print(\"Start Date:\", result[0]['minDate'])\n",
    "print(\"End Date:\", result[0]['maxDate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1b: filter for just a week of data (week 36 of 2017) Start: Monday, September 4th, 2017\n",
    "od_matrix_week36 = np.zeros((len(zones), len(zones)), dtype=int)\n",
    "for i, origin in enumerate(zones):\n",
    "    result = []  \n",
    "    for j, dest in enumerate(zones):\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"week\": { \"$week\": \"$init_date\" },\n",
    "                    \"day\": { \"$dayOfWeek\": \"$init_date\" },\n",
    "                    \"year\": { \"$year\": \"$init_date\" }, \n",
    "                    \"init_loc\": 1,\n",
    "                    \"final_loc\": 1\n",
    "                } \n",
    "            },\n",
    "            {\n",
    "                \"$match\": { \n",
    "                    \"day\": { \n",
    "                        \"$gt\": 1, \n",
    "                        \"$lt\":7 },\n",
    "                    \"week\": { \"$eq\": 36 },\n",
    "                    \"year\": { \"$eq\": 2017 },\n",
    "                    \"init_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": origin \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": dest  \n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$count\": \"tot\"  \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        aggregation_result = list(Bookings_col.aggregate(pipeline))\n",
    "        if aggregation_result:\n",
    "            od_matrix_week36[i, j] = aggregation_result[0][\"tot\"]\n",
    "        else:\n",
    "            od_matrix_week36[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = np.sum(od_matrix_week36)\n",
    "od_matrix_week36_norm = od_matrix_week36 / tot_sum\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(od_matrix_week36_norm, cmap=\"YlGnBu\", cbar=True, xticklabels=range(1, len(zones)+1), yticklabels=range(1, len(zones)+1))\n",
    "\n",
    "plt.title(\"OD Matrix Week 36 of 2017 Heat Map\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1b: filter for just a week of data (week 39 of 2017) Start: Monday, September 25, 2017\n",
    "od_matrix_week39 = np.zeros((len(zones), len(zones)), dtype=int)\n",
    "for i, origin in enumerate(zones):\n",
    "    result = []  \n",
    "    for j, dest in enumerate(zones):\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"week\": { \"$week\": \"$init_date\" },\n",
    "                    \"day\": { \"$dayOfWeek\": \"$init_date\" },\n",
    "                    \"year\": { \"$year\": \"$init_date\" }, \n",
    "                    \"init_loc\": 1,\n",
    "                    \"final_loc\": 1\n",
    "                } \n",
    "            },\n",
    "            {\n",
    "                \"$match\": { \n",
    "                    \"day\": { \n",
    "                        \"$gt\": 1, \n",
    "                        \"$lt\":7 },\n",
    "                    \"week\": { \"$eq\": 39 },\n",
    "                    \"year\": { \"$eq\": 2017 },\n",
    "                    \"init_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": origin \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": dest  \n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$count\": \"tot\"  \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        aggregation_result = list(Bookings_col.aggregate(pipeline))\n",
    "        if aggregation_result:\n",
    "            od_matrix_week39[i, j] = aggregation_result[0][\"tot\"]\n",
    "        else:\n",
    "            od_matrix_week39[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = np.sum(od_matrix_week39)\n",
    "od_matrix_week39_norm = od_matrix_week39 / tot_sum\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(od_matrix_week39_norm, cmap=\"YlGnBu\", cbar=True, xticklabels=range(1, len(zones)+1), yticklabels=range(1, len(zones)+1))\n",
    "\n",
    "plt.title(\"OD Matrix Week 39 of 2017 Heat Map\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1b: filter for just a week of data (week 39 of 2017) Start: Monday, September 25, 2017\n",
    "od_matrix_week39_enjoy = np.zeros((len(zones), len(zones)), dtype=int)\n",
    "for i, origin in enumerate(zones):\n",
    "    result = []  \n",
    "    for j, dest in enumerate(zones):\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"week\": { \"$week\": \"$init_date\" },\n",
    "                    \"day\": { \"$dayOfWeek\": \"$init_date\" },\n",
    "                    \"year\": { \"$year\": \"$init_date\" }, \n",
    "                    \"init_loc\": 1,\n",
    "                    \"final_loc\": 1\n",
    "                } \n",
    "            },\n",
    "            {\n",
    "                \"$match\": { \n",
    "                    \"day\": { \n",
    "                        \"$gt\": 1, \n",
    "                        \"$lt\":7 },\n",
    "                    \"week\": { \"$eq\": 39 },\n",
    "                    \"year\": { \"$eq\": 2017 },\n",
    "                    \"init_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": origin \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_loc\": {\n",
    "                        \"$geoWithin\": {\n",
    "                            \"$geometry\": {\n",
    "                                \"type\": \"MultiPolygon\",\n",
    "                                \"coordinates\": dest  \n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$count\": \"tot\"  \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        aggregation_result = list(Enjoy_Bookings_col.aggregate(pipeline))\n",
    "        if aggregation_result:\n",
    "            od_matrix_week39_enjoy[i, j] = aggregation_result[0][\"tot\"]\n",
    "        else:\n",
    "            od_matrix_week39_enjoy[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = np.sum(od_matrix_week39_enjoy)\n",
    "od_matrix_week39_enjoy_norm = od_matrix_week39_enjoy / tot_sum\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(od_matrix_week39_enjoy_norm, cmap=\"YlGnBu\", cbar=True, xticklabels=range(1, len(zones)+1), yticklabels=range(1, len(zones)+1))\n",
    "\n",
    "plt.title(\"OD Matrix Week 39 of 2017 Heat Map Enjoy\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_matrix_A = np.random.rand(23, 23)\n",
    "rand_matrix_A = rand_matrix_A / np.sum(rand_matrix_A)\n",
    "\n",
    "rand_matrix_B = np.random.rand(23, 23)\n",
    "rand_matrix_B = rand_matrix_B / np.sum(rand_matrix_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_distance(matrix1, matrix2):\n",
    "    return np.sum(np.abs(matrix1 - matrix2))\n",
    "\n",
    "d1_rand = simple_distance(rand_matrix_A, rand_matrix_B)\n",
    "print(f\"Absolute distance between random matrixes: {d1_rand}\")\n",
    "d1_weeks = simple_distance(od_matrix_week36_norm, od_matrix_week39_norm)\n",
    "print(f\"Absolute distance between week 36 and 39: {d1_weeks}\")\n",
    "d1_platfrom = simple_distance(od_matrix_week39_norm, od_matrix_week39_enjoy_norm)\n",
    "print(f\"Absolute distance between Car2go and Enjoy: {d1_platfrom}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(matrix1, matrix2):\n",
    "    return np.sqrt(np.sum((matrix1 - matrix2)**2))\n",
    "\n",
    "d2_rand = euclidean_distance(rand_matrix_A, rand_matrix_B)\n",
    "print(f\"Euclidean distance between random matrixes: {d2_rand}\")\n",
    "d2_weeks = euclidean_distance(od_matrix_week36_norm, od_matrix_week39_norm)\n",
    "print(f\"Euclidean distance between week 36 and 39: {d2_weeks}\")\n",
    "d2_platfrom = euclidean_distance(od_matrix_week39_norm, od_matrix_week39_enjoy_norm)\n",
    "print(f\"Euclidean distance between Car2go and Enjoy: {d2_platfrom}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_absolute_difference(A, B):\n",
    "    absolute_diff = np.abs(A - B)\n",
    "    return np.max(absolute_diff)\n",
    "\n",
    "dMax_rand = max_absolute_difference(rand_matrix_A, rand_matrix_B)\n",
    "print(f\"Max absolute difference between random matrixes: {dMax_rand}\")\n",
    "dMax_weeks = max_absolute_difference(od_matrix_week36_norm, od_matrix_week39_norm)\n",
    "print(f\"Max absolute difference between week 36 and 39: {dMax_weeks}\")\n",
    "dMax_platfrom = max_absolute_difference(od_matrix_week39_norm, od_matrix_week39_enjoy_norm)\n",
    "print(f\"Max absolute difference between Car2go and Enjoy: {dMax_platfrom}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dm(A, B):\n",
    "    M = A - B\n",
    "    n = M.shape[0]  # Size of the matrix\n",
    "    x = np.random.rand(n)  # Random vector\n",
    "    x /= np.linalg.norm(x)  # Normalize it to have ||x|| = 1\n",
    "    for _ in range(100):\n",
    "        x = M.T @ (M @ x)  # Multiply by M.T @ M\n",
    "        x /= np.linalg.norm(x)  # Normalize to ensure ||x|| = 1\n",
    "    max_norm = np.linalg.norm(M @ x)\n",
    "    return max_norm\n",
    "\n",
    "dNorm_rand = compute_dm(rand_matrix_A, rand_matrix_B)\n",
    "print(f\"Max norm of the difference between random matrixes: {dNorm_rand}\")\n",
    "dNorm_weeks = compute_dm(od_matrix_week36_norm, od_matrix_week39_norm)\n",
    "print(f\"Max norm of the difference between week 36 and 39: {dNorm_weeks}\")\n",
    "dNorm_platfrom = compute_dm(od_matrix_week39_norm, od_matrix_week39_enjoy_norm)\n",
    "print(f\"Max norm of the difference between Car2go and Enjoy: {dNorm_platfrom}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_dm2(A, B, iterations=100, tol=1e-9):\n",
    "    # Step 1: Compute the difference matrix M = A - B\n",
    "    M = A - B\n",
    "    n = M.shape[0]  # Number of rows/columns (assuming square matrix)\n",
    "    \n",
    "    # Step 2: Start with a random vector x\n",
    "    x = np.random.rand(n)\n",
    "    x /= np.linalg.norm(x)  # Normalize x to have ||x|| = 1\n",
    "    \n",
    "    # Step 3: Perform power iteration\n",
    "    prev_norm = 0\n",
    "    for _ in range(iterations):\n",
    "        # Multiply M by the current vector x\n",
    "        x = M @ x\n",
    "        \n",
    "        # Compute the 2-norm of the resulting vector\n",
    "        current_norm = np.linalg.norm(x)\n",
    "        \n",
    "        # Normalize x for the next iteration\n",
    "        x /= current_norm\n",
    "        \n",
    "        # Check for convergence\n",
    "        if abs(current_norm - prev_norm) < tol:\n",
    "            break\n",
    "        prev_norm = current_norm\n",
    "    \n",
    "    return current_norm\n",
    "\n",
    "dNorm_rand2 = compute_dm2(rand_matrix_A, rand_matrix_B)\n",
    "print(f\"Max norm of the difference between random matrixes: {dNorm_rand2}\")\n",
    "dNorm_weeks2 = compute_dm2(od_matrix_week36_norm, od_matrix_week39_norm)\n",
    "print(f\"Max norm of the difference between week 36 and 39: {dNorm_weeks2}\")\n",
    "dNorm_platfrom2 = compute_dm2(od_matrix_week39_norm, od_matrix_week39_enjoy_norm)\n",
    "print(f\"Max norm of the difference between Car2go and Enjoy: {dNorm_platfrom2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3\n",
    "\n",
    "\n",
    "unipol_info = pd.read_csv('./data/Unipoldata/Info_TO.csv')\n",
    "unipol_trips = pd.read_csv('./data/Unipoldata/Trips_OD_TO.csv')\n",
    "imq_data = pd.read_csv('./data/IMQ/IMQData_Torino.csv')\n",
    "\n",
    "unipol_data = pd.merge(unipol_trips, unipol_info, on='id_veicolo')\n",
    "\n",
    "print(\"UnipolTech Data:\")\n",
    "print(unipol_data.head())\n",
    "print(\"IMQ Data:\")\n",
    "print(imq_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon, mapping\n",
    "import json\n",
    "\n",
    "\n",
    "with open('./data/IMQ/Zone/TorinoZonescol.geojson') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "zones = []\n",
    "for feature in data['features']:\n",
    "    coordinates = feature['geometry']['coordinates']\n",
    "    if feature['geometry']['type'] == 'Polygon':\n",
    "        zones.append(Polygon(coordinates[0]))\n",
    "    elif feature['geometry']['type'] == 'MultiPolygon':\n",
    "        for polygon in coordinates:\n",
    "            zones.append(Polygon(polygon[0]))\n",
    "\n",
    "\n",
    "zone_gdf = gpd.GeoDataFrame(geometry=zones)\n",
    "\n",
    "\n",
    "def find_zone(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for i, zone in enumerate(zones):\n",
    "        if zone.contains(point):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "unipol_data['ORI_ZONE'] = unipol_data.apply(lambda row: find_zone(row['lat_start'], row['lon_start']), axis=1)\n",
    "unipol_data['DST_ZONE'] = unipol_data.apply(lambda row: find_zone(row['lat_stop'], row['lon_stop']), axis=1)\n",
    "\n",
    "unipol_data = unipol_data[(unipol_data['ORI_ZONE'] != -1) & (unipol_data['DST_ZONE'] != -1)]\n",
    "\n",
    "imq_data['ORI_ZONE'] = imq_data['ORI_ZONE'].str.replace('Q', '').astype(int)\n",
    "imq_data['DST_ZONE'] = imq_data['DST_ZONE'].str.replace('Q', '').astype(int)\n",
    "\n",
    "imq_data = imq_data[(imq_data['ORI_ZONE'] != -1) & (imq_data['DST_ZONE'] != -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_zone_index = 23\n",
    "od_matrix_unipol = np.zeros((max_zone_index  , max_zone_index ), dtype=int)\n",
    "od_matrix_imq = np.zeros((max_zone_index , max_zone_index ), dtype=int)\n",
    "\n",
    "for _, row in unipol_data.iterrows():\n",
    "    if 1 <= row['ORI_ZONE'] <= max_zone_index and 1 <= row['DST_ZONE'] <= max_zone_index:\n",
    "        od_matrix_unipol[row['ORI_ZONE'] -1, row['DST_ZONE']-1 ] += 1\n",
    "\n",
    "# Compute the OD matrix for IMQ\n",
    "for _, row in imq_data.iterrows():\n",
    "    if 1 <= row['ORI_ZONE'] <= max_zone_index and 1 <= row['DST_ZONE'] <= max_zone_index:\n",
    "        od_matrix_imq[row['ORI_ZONE']-1 , row['DST_ZONE'] -1] += 1\n",
    "\n",
    "od_matrix_unipol_normalized = od_matrix_unipol / od_matrix_unipol.sum()\n",
    "od_matrix_imq_normalized = od_matrix_imq / od_matrix_imq.sum()\n",
    "\n",
    "od_matrix_unipol_flat = od_matrix_unipol_normalized.flatten()\n",
    "od_matrix_imq_flat = od_matrix_imq_normalized.flatten()\n",
    "\n",
    "distance = compute_dm2(od_matrix_unipol_normalized, od_matrix_imq_normalized)\n",
    "print(f\"Distance between normalized UnipolTech and IMQ OD matrices: {distance}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(od_matrix_unipol_normalized, annot=True, fmt=\".2f\", cmap=\"viridis\",xticklabels=range(1, 24), yticklabels=range(1, 24))\n",
    "plt.title(\"Normalized OD Matrix UnipolTech\")\n",
    "plt.xlabel(\"Destination Zone\")\n",
    "plt.ylabel(\"Origin Zone\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(od_matrix_imq_normalized, annot=True, fmt=\".2f\", cmap=\"viridis\",xticklabels=range(1, 24), yticklabels=range(1, 24))\n",
    "plt.title(\"Normalized OD Matrix IMQ\")\n",
    "plt.xlabel(\"Destination Zone\")\n",
    "plt.ylabel(\"Origin Zone\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3a\n",
    "unipol_groups = unipol_data.groupby(['genere', 'eta_approx', 'commerciale'])\n",
    "\n",
    "imq_groups = imq_data.groupby(['GENDER', 'AGE'])\n",
    "\n",
    "max_zone_index = 23\n",
    "def compute_od_matrix(group, max_zone_index):\n",
    "    od_matrix = np.zeros((max_zone_index , max_zone_index ), dtype=int)\n",
    "    for _, row in group.iterrows():\n",
    "        if row['ORI_ZONE'] <= max_zone_index and row['DST_ZONE'] <= max_zone_index:\n",
    "            od_matrix[row['ORI_ZONE']-1, row['DST_ZONE']-1] += 1\n",
    "    return od_matrix\n",
    "\n",
    "unipol_od_matrices = {name: compute_od_matrix(group, max_zone_index) for name, group in unipol_groups}\n",
    "\n",
    "imq_od_matrices = {name: compute_od_matrix(group, max_zone_index) for name, group in imq_groups}\n",
    "\n",
    "def normalize_od_matrix(od_matrix):\n",
    "    return od_matrix / od_matrix.sum()\n",
    "\n",
    "unipol_od_matrices_normalized = {name: normalize_od_matrix(matrix) for name, matrix in unipol_od_matrices.items()}\n",
    "\n",
    "imq_od_matrices_normalized = {name: normalize_od_matrix(matrix) for name, matrix in imq_od_matrices.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "similarities = {}\n",
    "for unipol_name, unipol_matrix in unipol_od_matrices_normalized.items():\n",
    "    for imq_name, imq_matrix in imq_od_matrices_normalized.items():\n",
    "        similarity = compute_dm2(unipol_matrix, imq_matrix)\n",
    "        similarities[(unipol_name, imq_name)] = similarity\n",
    "\n",
    "for (unipol_name, imq_name), similarity in similarities.items():\n",
    "    unipol_gender, unipol_age, unipol_commercial = unipol_name\n",
    "    imq_gen, imq_age = imq_name\n",
    "    print(f\"Similarity between UnipolTech group (Gender: {unipol_gender}, Age: {unipol_age}, Commercial: {unipol_commercial}) \"\n",
    "          f\"and IMQ group (Gender: {imq_gen}, age: {imq_age}): {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unipol_od_matrices_normalized = {\n",
    "    (gender, int(age), commercial): matrix\n",
    "    for (gender, age, commercial), matrix in unipol_od_matrices_normalized.items()\n",
    "}\n",
    "\n",
    "imq_od_matrices_normalized = {\n",
    "    (int(gen), int(age)): matrix\n",
    "    for (gen, age), matrix in imq_od_matrices_normalized.items()\n",
    "}\n",
    "\n",
    "\n",
    "selected_unipol_group = ('F', 47, 'N')  #gender, age , commercial\n",
    "selected_imq_group = (1,4)  # gender - age\n",
    "\n",
    "if selected_unipol_group not in unipol_od_matrices_normalized:\n",
    "    print(f\"UnipolTech group {selected_unipol_group} does not exist.\")\n",
    "else:\n",
    "    print(f\"Selected UnipolTech Group: {selected_unipol_group}\")\n",
    "\n",
    "if selected_imq_group not in imq_od_matrices_normalized:\n",
    "    print(f\"IMQ group {selected_imq_group} does not exist.\")\n",
    "else:\n",
    "    print(f\"Selected IMQ Group: {selected_imq_group}\")\n",
    "\n",
    "if selected_unipol_group in unipol_od_matrices_normalized:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(unipol_od_matrices_normalized[selected_unipol_group], annot=True, fmt=\".2f\", cmap=\"viridis\", xticklabels=range(1, 24), yticklabels=range(1, 24))\n",
    "    plt.title(f\"Normalized OD Matrix UnipolTech Group {selected_unipol_group}\")\n",
    "    plt.xlabel(\"Destination Zone\")\n",
    "    plt.ylabel(\"Origin Zone\")\n",
    "\n",
    "if selected_imq_group in imq_od_matrices_normalized:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(imq_od_matrices_normalized[selected_imq_group], annot=True, fmt=\".2f\", cmap=\"viridis\", xticklabels=range(1, 24), yticklabels=range(1, 24))\n",
    "    plt.title(f\"Normalized OD Matrix IMQ Group {selected_imq_group}\")\n",
    "    plt.xlabel(\"Destination Zone\")\n",
    "    plt.ylabel(\"Origin Zone\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_differences = {}\n",
    "all_differences = {}\n",
    "max_difference_age_groups = {}\n",
    "\n",
    "for gender in imq_data['GENDER'].unique():\n",
    "    age_groups = [key for key in imq_od_matrices_normalized.keys() if key[0] == gender]\n",
    "    differences = []\n",
    "    age_group_pairs = []\n",
    "    for i in range(len(age_groups)):\n",
    "        for j in range(i + 1, len(age_groups)):\n",
    "            age_group1 = age_groups[i]\n",
    "            age_group2 = age_groups[j]\n",
    "            similarity = compute_dm2(imq_od_matrices_normalized[age_group1], imq_od_matrices_normalized[age_group2])\n",
    "            differences.append(similarity)\n",
    "            age_group_pairs.append((age_group1[1], age_group2[1]))\n",
    "    all_differences[gender] = list(zip(age_group_pairs, differences))\n",
    "    max_difference_index = np.argmax(differences)\n",
    "    gender_differences[gender] = differences[max_difference_index]\n",
    "    max_difference_age_groups[gender] = age_group_pairs[max_difference_index]\n",
    "\n",
    "\n",
    "for gender, diffs in all_differences.items():\n",
    "    print(f\"Differences for gender {gender}:\")\n",
    "    for (age_group1, age_group2), difference in diffs:\n",
    "        print(f\"  Between ages {age_group1} and {age_group2}: {difference}\")\n",
    "\n",
    "\n",
    "for gender, difference in gender_differences.items():\n",
    "    age_group1, age_group2 = max_difference_age_groups[gender]\n",
    "    print(f\"Maximum difference in behavior across age groups for gender {gender}: {difference} (between ages {age_group1} and {age_group2})\")\n",
    "\n",
    "max_difference_gender = max(gender_differences, key=gender_differences.get)\n",
    "print(f\"Gender with greater differences in behavior across age groups: {max_difference_gender}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car2go_data = pd.DataFrame(list(Bookings_col.find()))\n",
    "enjoy_data = pd.DataFrame(list(Enjoy_Bookings_col.find()))\n",
    "\n",
    "with open('./data/IMQ/Zone/TorinoZonescol.geojson') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "zones = []\n",
    "for feature in data['features']:\n",
    "    coordinates = feature['geometry']['coordinates']\n",
    "    if feature['geometry']['type'] == 'Polygon':\n",
    "        zones.append(Polygon(coordinates[0]))\n",
    "    elif feature['geometry']['type'] == 'MultiPolygon':\n",
    "        for polygon in coordinates:\n",
    "            zones.append(Polygon(polygon[0]))\n",
    "\n",
    "\n",
    "zone_gdf = gpd.GeoDataFrame(geometry=zones)\n",
    "\n",
    "\n",
    "def find_zone(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for i, zone in enumerate(zones):\n",
    "        if zone.contains(point):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def compute_od_matrix(group, max_zone_index):\n",
    "    od_matrix = np.zeros((max_zone_index + 1, max_zone_index + 1), dtype=int)\n",
    "    for _, row in group.iterrows():\n",
    "        if row['origin'] <= max_zone_index and row['dest'] <= max_zone_index:\n",
    "            od_matrix[row['origin'], row['dest']] += 1\n",
    "    return od_matrix\n",
    "\n",
    "\n",
    "\n",
    "car2go_od_matrix_normalized = normalize_od_matrix(od_matrix)\n",
    "enjoy_od_matrix_normalized = normalize_od_matrix(od_matrix_e)\n",
    "\n",
    "similarities = {}\n",
    "for name, matrix in imq_od_matrices_normalized.items():\n",
    "    car2go_similarity = compute_dm2(matrix, car2go_od_matrix_normalized)\n",
    "    enjoy_similarity = compute_dm2(matrix, enjoy_od_matrix_normalized)\n",
    "    similarities[name] = {'Car2GO': car2go_similarity, 'Enjoy': enjoy_similarity}\n",
    "\n",
    "for name, sim in similarities.items():\n",
    "    print(f\"Similarity for group {name}: Car2GO = {sim['Car2GO']}, Enjoy = {sim['Enjoy']}\")\n",
    "\n",
    "max_car2go_similarity = min(similarities.items(), key=lambda x: x[1]['Car2GO'])\n",
    "max_enjoy_similarity = min(similarities.items(), key=lambda x: x[1]['Enjoy'])\n",
    "\n",
    "print(f\"Maximum similarity for Car2GO: Group {max_car2go_similarity[0]} with similarity {max_car2go_similarity[1]['Car2GO']}\")\n",
    "print(f\"Maximum similarity for Enjoy: Group {max_enjoy_similarity[0]} with similarity {max_enjoy_similarity[1]['Enjoy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "The idea is to use the KDE and estimate the 20 points of the city of Turin with highest densityof trips as hospot and then use Voronoi clustering to create zones around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo as pm\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial import Voronoi\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# MongoDB Connection\n",
    "client = pm.MongoClient(\n",
    "    'bigdatadb.polito.it',\n",
    "    ssl=True,\n",
    "    authSource='carsharing',\n",
    "    username='ictts',\n",
    "    password='Ict4SM22!',\n",
    "    tlsAllowInvalidCertificates=True\n",
    ")\n",
    "\n",
    "# Database and Collections\n",
    "db = client['carsharing']\n",
    "car2go_col = db['ictts_PermanentBookings']\n",
    "enjoy_col = db['ictts_enjoy_PermanentBookings']\n",
    "\n",
    "# Load CarSharing Trips\n",
    "def load_trips(collection):\n",
    "    return [\n",
    "        {\"latitude\": doc[\"init_loc\"][\"coordinates\"][1], \"longitude\": doc[\"init_loc\"][\"coordinates\"][0]}\n",
    "        for doc in collection.find({\"city\": \"Torino\"}) if 'init_time' in doc and 'final_time' in doc\n",
    "    ]\n",
    "\n",
    "car2go_trips = load_trips(car2go_col)\n",
    "enjoy_trips = load_trips(enjoy_col)\n",
    "unipol_df = pd.read_csv(\"Trips_OD_TO.csv\")\n",
    "\n",
    "# Estrarre coordinate di origine\n",
    "unipol_trips = [\n",
    "    {\"latitude\": row[\"lat_start\"], \"longitude\": row[\"lon_start\"]}\n",
    "    for _, row in unipol_df.iterrows()\n",
    "]\n",
    "# Combine and Convert to GeoDataFrame\n",
    "trips_data = pd.DataFrame(car2go_trips + enjoy_trips)\n",
    "trips_gdf = gpd.GeoDataFrame(\n",
    "    trips_data, geometry=gpd.points_from_xy(trips_data[\"longitude\"], trips_data[\"latitude\"])\n",
    ")\n",
    "trips_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "# Save to GeoJSON\n",
    "trips_gdf.to_file(\"Turin_trips.geojson\", driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import contextily as ctx\n",
    "\n",
    "def find_high_density_hotspots(trips_gdf, kde_bandwidth=0.1, num_hotspots=20):\n",
    "    trip_coords = np.array([[p.x, p.y] for p in trips_gdf.geometry])  # (lon, lat)\n",
    "    kde = gaussian_kde(trip_coords.T, bw_method=kde_bandwidth)  # Perform KDE\n",
    "    hotspot_scores = kde(trip_coords.T)\n",
    "    hotspot_indices = np.argsort(hotspot_scores)[-num_hotspots:]  # Top hotspots\n",
    "    hotspots = trips_gdf.iloc[hotspot_indices]\n",
    "    return hotspots\n",
    "\n",
    "# Convert to Web Mercator (EPSG:3857)\n",
    "trips_gdf.crs = \"EPSG:4326\"  # Set initial CRS to WGS84\n",
    "trips_gdf = trips_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Remove rows with NaN or infinite values\n",
    "#trips_gdf = trips_gdf.replace([np.inf, -np.inf], np.nan).dropna(subset=['geometry'])\n",
    "\n",
    "# Extract key locations for Voronoi Tessellation (e.g., city landmarks or trip hotspots)\n",
    "#key_locations = find_high_density_hotspots(trips_gdf)\n",
    "# Test with the first 1000 rows of trips_gdf\n",
    "#test_trips_gdf = trips_gdf.head(10000)\n",
    "key_locations_test = find_high_density_hotspots(trips_gdf)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "trips_gdf.plot(\n",
    "   color='blue',  \n",
    "   markersize=3,\n",
    "   ax=ax,\n",
    "   legend=True,\n",
    "    alpha=0.7,\n",
    "    label='Trips'\n",
    ")\n",
    "\n",
    "key_locations_test.plot(\n",
    "    color='maroon', \n",
    "    markersize=20,\n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    alpha=0.7,\n",
    "    label='Hotspots'\n",
    ")\n",
    "\n",
    "ctx.add_basemap(ax,zoom = 14, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "plt.title(\"High Density Hotspots\")\n",
    "plt.savefig(\"High Density Hotspots2.png\", dpi=300)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the hotspots\n",
    "#fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#ax.scatter(test_trips_gdf.geometry.x, test_trips_gdf.geometry.y, s=2, label=\"Trips\")\n",
    "#ax.scatter(key_locations_test.geometry.x, key_locations_test.geometry.y, color='red', s=50, label=\"Hotspots\")\n",
    "#ax.set_title(\"High Density Hotspots\")\n",
    "#ax.set_xlabel(\"Longitude\")\n",
    "#ax.set_ylabel(\"Latitude\")\n",
    "#ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "#ax.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import Voronoi\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "# Funzione per convertire Voronoi Tessellation in GeoDataFrame\n",
    "def voronoi_to_geodataframe(vor, bounds_polygon):\n",
    "    regions = []\n",
    "    for region_idx in vor.point_region:\n",
    "        region = vor.regions[region_idx]\n",
    "        if -1 not in region and len(region) > 0:\n",
    "            polygon = Polygon([vor.vertices[i] for i in region])\n",
    "            polygon = polygon.intersection(bounds_polygon)\n",
    "            if not polygon.is_empty:\n",
    "                regions.append(polygon)\n",
    "    return gpd.GeoDataFrame(geometry=regions, crs=\"EPSG:3857\")\n",
    "\n",
    "# Estrazione delle coordinate dei viaggi\n",
    "trip_coords = np.array([[p.x, p.y] for p in trips_gdf.geometry])\n",
    "\n",
    "# Applicazione del K-Means clustering con k=20\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "kmeans.fit(trip_coords)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Bounding box per la tessellation\n",
    "bounds = trips_gdf.total_bounds\n",
    "bounds_polygon = Polygon([\n",
    "    (bounds[0], bounds[1]), (bounds[0], bounds[3]),\n",
    "    (bounds[2], bounds[3]), (bounds[2], bounds[1]), (bounds[0], bounds[1])\n",
    "])\n",
    "\n",
    "# Creazione della Voronoi Tessellation\n",
    "vor = Voronoi(cluster_centers)\n",
    "voronoi_gdf = voronoi_to_geodataframe(vor, bounds_polygon)\n",
    "\n",
    "# Creazione GeoDataFrame per i centri dei cluster\n",
    "centers_gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(cluster_centers[:, 0], cluster_centers[:, 1]), crs=\"EPSG:4326\")\n",
    "\n",
    "# Plot della Voronoi Tessellation con K-Means cluster centers\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot delle zone Voronoi\n",
    "voronoi_gdf.plot(ax=ax, color=\"lightblue\", alpha=0.5, edgecolor=\"black\", label=\"Voronoi Zones\")\n",
    "\n",
    "# Plot dei punti K-Means (centriideali)\n",
    "centers_gdf.plot(ax=ax, color=\"red\", markersize=50, label=\"Cluster Centers\")\n",
    "\n",
    "# Plot dei dati dei viaggi\n",
    "trips_gdf.plot(ax=ax, color=\"gray\", markersize=1, alpha=0.3, label=\"Trips\")\n",
    "\n",
    "# Aggiunta della basemap\n",
    "ctx.add_basemap(ax, crs=trips_gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# Dettagli del plot\n",
    "ax.set_title(\"Voronoi Tessellation Based on K-Means Cluster Centers\", fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"K-Means_clustering.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.spatial import Voronoi\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "# 1. Definizione dei landmarks con coordinate geografiche (lat, lon)\n",
    "landmarks = [\n",
    "    {\"name\": \"Piazza Castello\", \"lon\": 7.6868, \"lat\": 45.0705},\n",
    "    {\"name\": \"Mole Antonelliana\", \"lon\": 7.6869, \"lat\": 45.0704},\n",
    "    {\"name\": \"Piazza San Carlo\", \"lon\": 7.6837, \"lat\": 45.0676},\n",
    "    {\"name\": \"Stazione Porta Nuova\", \"lon\": 7.6787, \"lat\": 45.0616},\n",
    "    {\"name\": \"Stazione Porta Susa\", \"lon\": 7.6667, \"lat\": 45.0712},\n",
    "    {\"name\": \"Politecnico di Torino\", \"lon\": 7.6616, \"lat\": 45.0635},\n",
    "    {\"name\": \"Università degli Studi di Torino\", \"lon\": 7.6867, \"lat\": 45.0674},\n",
    "    {\"name\": \"Piazza Vittorio Veneto\", \"lon\": 7.6955, \"lat\": 45.0675},\n",
    "    #{\"name\": \"Gran Madre di Dio\", \"lon\": 7.7000, \"lat\": 45.0639},\n",
    "    {\"name\": \"Parco del Valentino\", \"lon\": 7.6822, \"lat\": 45.0515},\n",
    "    {\"name\": \"Lingotto Fiere\", \"lon\": 7.6697, \"lat\": 45.0355},\n",
    "    #{\"name\": \"Eataly Torino Lingotto\", \"lon\": 7.6693, \"lat\": 45.0352},\n",
    "    {\"name\": \"Juventus Stadium\", \"lon\": 7.6410, \"lat\": 45.1099},\n",
    "    {\"name\": \"Palazzo Madama\", \"lon\": 7.6868, \"lat\": 45.0703},\n",
    "    {\"name\": \"Museo Egizio\", \"lon\": 7.6867, \"lat\": 45.0698},\n",
    "    #{\"name\": \"Basilica di Superga\", \"lon\": 7.7670, \"lat\": 45.0748},\n",
    "    {\"name\": \"Ospedale Molinette\", \"lon\": 7.6727, \"lat\": 45.0434},\n",
    "    #{\"name\": \"Aeroporto di Torino Caselle\", \"lon\": 7.6496, \"lat\": 45.1923},\n",
    "    {\"name\": \"Stadio Olimpico Grande Torino\", \"lon\": 7.6503, \"lat\": 45.0411},\n",
    "    {\"name\": \"Lingotto (ex Fiat Factory)\", \"lon\": 7.6692, \"lat\": 45.0343},\n",
    "]\n",
    "\n",
    "# 2. Creazione di un GeoDataFrame dei landmarks\n",
    "landmarks_df = pd.DataFrame(landmarks)\n",
    "landmarks_gdf = gpd.GeoDataFrame(\n",
    "    landmarks_df, geometry=gpd.points_from_xy(landmarks_df[\"lon\"], landmarks_df[\"lat\"])\n",
    ")\n",
    "landmarks_gdf.crs = \"EPSG:4326\"\n",
    "#landmarks_gdf = landmarks_gdf.to_crs(epsg=3857)  # Conversione a EPSG:3857 per basemap\n",
    "\n",
    "# 3. Funzione per la Voronoi Tessellation con punti fittizi ai bordi\n",
    "def voronoi_to_geodataframe(vor, bounds_polygon):\n",
    "    regions = []\n",
    "    for region_idx in vor.point_region:\n",
    "        region = vor.regions[region_idx]\n",
    "        if -1 not in region and len(region) > 0:\n",
    "            polygon = Polygon([vor.vertices[i] for i in region])\n",
    "            polygon = polygon.intersection(bounds_polygon)\n",
    "            if not polygon.is_empty:\n",
    "                regions.append(polygon)\n",
    "    return gpd.GeoDataFrame(geometry=regions, crs=\"EPSG:4326\")\n",
    "\n",
    "def add_boundary_points(coords, bounds):\n",
    "    \"\"\"\n",
    "    Aggiunge punti fittizi ai bordi del bounding box.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bounds\n",
    "    boundary_points = [\n",
    "        [x_min, y_min], [x_min, y_max], [x_max, y_min], [x_max, y_max],\n",
    "        [(x_min + x_max) / 2, y_min], [(x_min + x_max) / 2, y_max], [x_min, (y_min + y_max) / 2], [x_max, (y_min + y_max) / 2]\n",
    "    ]\n",
    "    return np.vstack([coords, boundary_points])\n",
    "\n",
    "# 4. Creazione della tessellation con punti fittizi\n",
    "coords = np.array([[p.x, p.y] for p in landmarks_gdf.geometry])\n",
    "bounds = trips_gdf.total_bounds\n",
    "bounds_polygon = Polygon([\n",
    "    (bounds[0], bounds[1]), (bounds[0], bounds[3]),\n",
    "    (bounds[2], bounds[3]), (bounds[2], bounds[1]), (bounds[0], bounds[1])\n",
    "])\n",
    "coords_with_boundary = add_boundary_points(coords, bounds)\n",
    "vor = Voronoi(coords_with_boundary)\n",
    "voronoi_gdf = voronoi_to_geodataframe(vor, bounds_polygon)\n",
    "\n",
    "trips_gdf.crs = \"EPSG:4326\"\n",
    "#trips_gdf = trips_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# 6. Plot finale\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot delle zone Voronoi\n",
    "voronoi_gdf.plot(ax=ax, color=\"lightblue\", alpha=0.5, edgecolor=\"black\", label=\"Voronoi Zones\")\n",
    "\n",
    "# Plot dei dati dei viaggi (trasparenza e ordine)\n",
    "trips_gdf.plot(ax=ax, color=\"gray\", markersize=1, alpha=0.2, label=\"Trips\")\n",
    "\n",
    "# Plot dei landmarks\n",
    "landmarks_gdf.plot(ax=ax, color=\"red\", markersize=50, label=\"Landmarks\")\n",
    "\n",
    "# Aggiunta della basemap\n",
    "ctx.add_basemap(ax, crs=landmarks_gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# Miglioramenti estetici\n",
    "ax.set_title(\"Voronoi Tessellation with Landmarks and Trips\", fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Voronoi_with_Landmarks2.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.spatial import Voronoi\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "# 1. Definizione dei landmarks con coordinate geografiche (lat, lon)\n",
    "landmarks = [\n",
    "    {\"name\": \"Piazza Castello\", \"lon\": 7.6868, \"lat\": 45.0705},\n",
    "    {\"name\": \"Mole Antonelliana\", \"lon\": 7.6869, \"lat\": 45.0704},\n",
    "    {\"name\": \"Piazza San Carlo\", \"lon\": 7.6837, \"lat\": 45.0676},\n",
    "    {\"name\": \"Stazione Porta Nuova\", \"lon\": 7.6787, \"lat\": 45.0616},\n",
    "    {\"name\": \"Stazione Porta Susa\", \"lon\": 7.6667, \"lat\": 45.0712},\n",
    "    {\"name\": \"Politecnico di Torino\", \"lon\": 7.6616, \"lat\": 45.0635},\n",
    "    {\"name\": \"Piazza Vittorio Veneto\", \"lon\": 7.6955, \"lat\": 45.0675},\n",
    "    {\"name\": \"Juventus Stadium\", \"lon\": 7.6410, \"lat\": 45.1099},\n",
    "    {\"name\": \"Lingotto Fiere\", \"lon\": 7.6697, \"lat\": 45.0355},\n",
    "    #{\"name\": \"Basilica di Superga\", \"lon\": 7.7670, \"lat\": 45.0748},\n",
    "]\n",
    "\n",
    "# Creazione di un GeoDataFrame dei landmarks\n",
    "landmarks_df = pd.DataFrame(landmarks)\n",
    "landmarks_gdf = gpd.GeoDataFrame(\n",
    "    landmarks_df, geometry=gpd.points_from_xy(landmarks_df[\"lon\"], landmarks_df[\"lat\"])\n",
    ")\n",
    "landmarks_gdf.crs = \"EPSG:4326\"\n",
    "trips_gdf.crs = \"EPSG:4326\"\n",
    "trip_coords = np.array([[p.x, p.y] for p in trips_gdf.geometry])\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(trip_coords)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "hybrid_coords = np.vstack([cluster_centers, [[p.x, p.y] for p in landmarks_gdf.geometry]])\n",
    "vor = Voronoi(hybrid_coords)\n",
    "def voronoi_to_geodataframe(vor, bounds_polygon):\n",
    "    \"\"\"\n",
    "    Converte una Voronoi tessellation in un GeoDataFrame ritagliato sulla bounding box.\n",
    "    \"\"\"\n",
    "    regions = []\n",
    "    for region_idx in vor.point_region:\n",
    "        region = vor.regions[region_idx]\n",
    "        if -1 not in region and len(region) > 0:\n",
    "            # Crea il poligono dalla regione\n",
    "            polygon = Polygon([vor.vertices[i] for i in region])\n",
    "            # Ritaglia il poligono alla bounding box\n",
    "            polygon = polygon.intersection(bounds_polygon)\n",
    "            if not polygon.is_empty:\n",
    "                regions.append(polygon)\n",
    "    return gpd.GeoDataFrame(geometry=regions, crs=\"EPSG:4326\")\n",
    "\n",
    "# 6. Definizione della bounding box della città di Torino\n",
    "bounds = trips_gdf.total_bounds\n",
    "bounds_polygon = Polygon([\n",
    "    (bounds[0], bounds[1]), (bounds[0], bounds[3]),\n",
    "    (bounds[2], bounds[3]), (bounds[2], bounds[1]), (bounds[0], bounds[1])\n",
    "])\n",
    "\n",
    "# Creazione dei poligoni Voronoi ritagliati con precisione\n",
    "voronoi_gdf = voronoi_to_geodataframe(vor, bounds_polygon)\n",
    "\n",
    "# GeoDataFrame per punti ibridi (landmarks + centri K-Means)\n",
    "hybrid_gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(hybrid_coords[:, 0], hybrid_coords[:, 1]), crs=\"EPSG:4326\")\n",
    "\n",
    "# 7. Plot finale\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "voronoi_gdf.plot(ax=ax, color=\"lightblue\", alpha=0.5, edgecolor=\"black\", label=\"Voronoi Zones\")\n",
    "trips_gdf.plot(ax=ax, color=\"gray\", markersize=1, alpha=0.2, label=\"Trips\")\n",
    "hybrid_gdf.plot(ax=ax, color=\"red\", markersize=50, label=\"Hybrid Points\")\n",
    "ctx.add_basemap(ax, crs=landmarks_gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "ax.set_title(\"Hybrid approach for Voronoi Tessellation (K-means+Landmarks)\", fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Precise_Voronoi.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "The idea is to compute the KDE for both dataset and then use Correlation and MSE to compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# KDE and Quantitative Comparison\n",
    "def compare_kde(carsharing_gdf, unipoltech_csv, bounds):\n",
    "    # Load UnipolTech Trips\n",
    "    unipoltech = pd.read_csv(unipoltech_csv)\n",
    "    unipoltech[\"geometry_origin\"] = gpd.points_from_xy(unipoltech[\"lon_start\"], unipoltech[\"lat_start\"])\n",
    "    unipoltech[\"geometry_destination\"] = gpd.points_from_xy(unipoltech[\"lon_stop\"], unipoltech[\"lat_stop\"])\n",
    "    unipoltech_origins = gpd.GeoDataFrame(unipoltech, geometry=\"geometry_origin\")\n",
    "    unipoltech_destinations = gpd.GeoDataFrame(unipoltech, geometry=\"geometry_destination\")\n",
    "\n",
    "    # Prepare Coordinates\n",
    "    carsharing_origins = np.array([[p.x, p.y] for p in carsharing_gdf.geometry])\n",
    "    unipoltech_origins_coords = np.array([[p.x, p.y] for p in unipoltech_origins.geometry])\n",
    "\n",
    "    # KDE and Plotting\n",
    "    def estimate_kde(coords, title):\n",
    "        kde = gaussian_kde(coords.T, bw_method=0.1)\n",
    "        x, y = np.mgrid[bounds[0]:bounds[2]:100j, bounds[1]:bounds[3]:100j]\n",
    "        density = kde(np.vstack([x.ravel(), y.ravel()])).reshape(x.shape)\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.imshow(np.rot90(density), extent=bounds, cmap=\"coolwarm\", alpha=0.8)\n",
    "        ax.scatter(coords[:, 0], coords[:, 1], color=\"green\", s=2, label=\"Trips\")\n",
    "        ax.set_title(title)\n",
    "        plt.show()\n",
    "        return density\n",
    "\n",
    "    carsharing_density = estimate_kde(carsharing_origins, \"CarSharing - KDE\")\n",
    "    unipoltech_density = estimate_kde(unipoltech_origins_coords, \"UnipolTech - KDE\")\n",
    "\n",
    "    # Quantitative Comparison\n",
    "    correlation = np.corrcoef(carsharing_density.ravel(), unipoltech_density.ravel())[0, 1]\n",
    "    mse = mean_squared_error(carsharing_density.ravel(), unipoltech_density.ravel())\n",
    "    print(f\"Correlation: {correlation}, MSE: {mse}\")\n",
    "\n",
    "# Call the comparison function\n",
    "compare_kde(trips_gdf, \"data/Unipoldata/Trips_OD_TO.csv\", trips_gdf.total_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsharing_origins = trips_gdf  # Usato dal codice precedente\n",
    "\n",
    "# GeoDataFrame per UnipolTech (origine e destinazione)\n",
    "unipol_origins = gpd.GeoDataFrame(\n",
    "    unipol_df, geometry=gpd.points_from_xy(unipol_df[\"lon_start\"], unipol_df[\"lat_start\"]), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "unipol_destinations = gpd.GeoDataFrame(\n",
    "    unipol_df, geometry=gpd.points_from_xy(unipol_df[\"lon_stop\"], unipol_df[\"lat_stop\"]), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import numpy as np\n",
    "\n",
    "# Funzione per calcolare i valori KDE su una griglia\n",
    "# Funzione KDE aggiornata con bounding box locali\n",
    "def kde_to_heatmap_data_limited(gdf, bounds, grid_size=100, bandwidth=0.01):\n",
    "    coords = np.vstack([gdf.geometry.y, gdf.geometry.x])  # Folium usa lat, lon\n",
    "    kde = gaussian_kde(coords, bw_method=bandwidth)\n",
    "\n",
    "    # Griglia limitata alla bounding box\n",
    "    x_min, y_min, x_max, y_max = bounds\n",
    "    x, y = np.mgrid[y_min:y_max:grid_size*1j, x_min:x_max:grid_size*1j]\n",
    "    grid_coords = np.vstack([x.ravel(), y.ravel()])\n",
    "    density = kde(grid_coords).reshape(x.shape)\n",
    "\n",
    "    # Convertire la griglia in formato lat, lon e intensità\n",
    "    heatmap_data = []\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            heatmap_data.append([x[i, j], y[i, j], density[i, j]])\n",
    "    return heatmap_data\n",
    "\n",
    "# Calcolo della bounding box ristretta\n",
    "local_bounds = carsharing_origins.total_bounds  # Usa solo i dati validi di CarSharing\n",
    "\n",
    "# Generazione delle heatmap limitate\n",
    "carsharing_heatmap_data = kde_to_heatmap_data_limited(carsharing_origins, local_bounds)\n",
    "unipol_gen_heatmap_data = kde_to_heatmap_data_limited(unipol_origins, local_bounds)\n",
    "unipol_dist_heatmap_data = kde_to_heatmap_data_limited(unipol_destinations, local_bounds)\n",
    "\n",
    "# Plot delle heatmap con Folium\n",
    "carsharing_map = plot_folium_heatmap(carsharing_heatmap_data, \"CarSharing - Trip Generation Density\")\n",
    "unipol_gen_map = plot_folium_heatmap(unipol_gen_heatmap_data, \"UnipolTech - Trip Generation Density\")\n",
    "unipol_dist_map = plot_folium_heatmap(unipol_dist_heatmap_data, \"UnipolTech - Trip Distribution Density\")\n",
    "\n",
    "# Salva e mostra le mappe\n",
    "carsharing_map.save(\"carsharing_trip_generation_limited.html\")\n",
    "unipol_gen_map.save(\"unipoltech_trip_generation_limited.html\")\n",
    "unipol_dist_map.save(\"unipoltech_trip_distribution_limited.html\")\n",
    "\n",
    "carsharing_map\n",
    "\n",
    "\n",
    "# Output della mappa nel notebook\n",
    "carsharing_map\n",
    "\n",
    "\n",
    "# Confronto delle Densità\n",
    "def compare_densities(density1, density2, label1, label2):\n",
    "    correlation = np.corrcoef(density1.ravel(), density2.ravel())[0, 1]\n",
    "    mse = mean_squared_error(density1.ravel(), density2.ravel())\n",
    "    print(f\"{label1} vs {label2} - Correlation: {correlation:.4f}, MSE: {mse:.4f}\")\n",
    "\n",
    "compare_densities(carsharing_density, unipol_gen_density, \"CarSharing Generation\", \"UnipolTech Generation\")\n",
    "compare_densities(unipol_gen_density, unipol_dist_density, \"UnipolTech Generation\", \"UnipolTech Distribution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
